{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f97b76d",
   "metadata": {},
   "source": [
    "IMPORTS AND INITIAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965eb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "\n",
    "# Define a function to load and preprocess the data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from file and perform initial preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the data file\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(r\"C:\\Users\\ashwi\\OneDrive\\Desktop\\Sycamore\\Hackathons\\HackToFuture sjec\\ML part\\scheduling_dataset.json\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "        \n",
    "        # Fill missing values (customize this based on your data)\n",
    "        # Numeric columns: fill with median\n",
    "        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        for col in numeric_cols:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "                \n",
    "        # Categorical columns: fill with mode\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in cat_cols:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # Handle outliers in numeric columns (Optional)\n",
    "    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (1.5 * iqr)\n",
    "        upper_bound = q3 + (1.5 * iqr)\n",
    "        \n",
    "        # Cap outliers instead of removing them\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    print(\"\\nPreprocessing complete!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6184c441",
   "metadata": {},
   "source": [
    "Section 2: Class Balancing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5f4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle class imbalance\n",
    "def balance_classes(df, target_column='assignment_valid'):\n",
    "    \"\"\"\n",
    "    Handle class imbalance using upsampling.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target_column: Name of the target column\n",
    "        \n",
    "    Returns:\n",
    "        Balanced DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Balancing classes...\")\n",
    "    # Check class distribution\n",
    "    class_counts = df[target_column].value_counts()\n",
    "    print(f\"Original class distribution:\\n{class_counts}\")\n",
    "    \n",
    "    # Split into majority and minority\n",
    "    df_majority = df[df[target_column] == 0]\n",
    "    df_minority = df[df[target_column] == 1]\n",
    "    \n",
    "    # Upsample the minority class\n",
    "    df_minority_upsampled = resample(\n",
    "        df_minority,\n",
    "        replace=True,                      # Allow duplicates\n",
    "        n_samples=len(df_majority),        # Make it same size as majority\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Combine both\n",
    "    df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "    \n",
    "    # Shuffle so they're mixed well\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Check new class distribution\n",
    "    balanced_class_counts = df_balanced[target_column].value_counts()\n",
    "    print(f\"Balanced class distribution:\\n{balanced_class_counts}\")\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "# Feature engineering function\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create new features to improve model performance.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with new features\n",
    "    \"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Example: Create time-based features if timestamps exist\n",
    "    # Uncomment and modify based on your dataset structure\n",
    "    \"\"\"\n",
    "    if 'timestamp' in df_new.columns:\n",
    "        df_new['timestamp'] = pd.to_datetime(df_new['timestamp'])\n",
    "        df_new['hour_of_day'] = df_new['timestamp'].dt.hour\n",
    "        df_new['day_of_week'] = df_new['timestamp'].dt.dayofweek\n",
    "        df_new['is_weekend'] = df_new['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example: Create interaction features\n",
    "    # Uncomment and modify based on your dataset structure\n",
    "    \"\"\"\n",
    "    if 'experience_years' in df_new.columns and 'project_count' in df_new.columns:\n",
    "        df_new['exp_per_project'] = df_new['experience_years'] / (df_new['project_count'] + 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add your custom feature engineering code here based on your data\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b766192",
   "metadata": {},
   "source": [
    "Section 3: Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea84e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "def build_and_train_model(df, target_column='assignment_valid', test_size=0.2, tune_hyperparams=True):\n",
    "    \"\"\"\n",
    "    Build, train and evaluate the random forest model.\n",
    "    \n",
    "    Args:\n",
    "        df: Preprocessed DataFrame\n",
    "        target_column: Name of the target column\n",
    "        test_size: Proportion of data to use for testing\n",
    "        tune_hyperparams: Whether to perform hyperparameter tuning\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, feature names, and evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"Building and training model...\")\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    print(f\"Numeric features: {len(numeric_features)}\")\n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    # Create train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Initial Random Forest settings\n",
    "    base_rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Create pipeline\n",
    "    rf_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', base_rf)\n",
    "    ])\n",
    "    \n",
    "    # Train the model - with or without hyperparameter tuning\n",
    "    if tune_hyperparams:\n",
    "        print(\"Performing hyperparameter tuning (this may take a while)...\")\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            rf_pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        \n",
    "        # Use the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "    else:\n",
    "        print(\"Training model without hyperparameter tuning...\")\n",
    "        rf_pipeline.fit(X_train, y_train)\n",
    "        best_model = rf_pipeline\n",
    "    \n",
    "    # Get feature names after one-hot encoding\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add numeric feature names directly\n",
    "    feature_names.extend(numeric_features.tolist())\n",
    "    \n",
    "    # Get one-hot encoded feature names if there are categorical features\n",
    "    if len(categorical_features) > 0:\n",
    "        ohe = best_model.named_steps['preprocessor'].named_transformers_['cat']\n",
    "        cat_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "        feature_names.extend(cat_feature_names.tolist())\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"\\nAUC-ROC Score: {auc_roc:.4f}\")\n",
    "    \n",
    "    # Create evaluation dictionary for later reference\n",
    "    evaluation = {\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'auc_roc': auc_roc,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    return best_model, feature_names, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04b26b",
   "metadata": {},
   "source": [
    "Section 4: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b48dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze feature importance\n",
    "def analyze_feature_importance(model, feature_names, X_test, y_test, top_n=20):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature importance.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        feature_names: List of feature names\n",
    "        X_test: Test features\n",
    "        y_test: Test target\n",
    "        top_n: Number of top features to display\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with feature importances\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing top {top_n} feature importances...\")\n",
    "    \n",
    "    # Extract the random forest classifier from the pipeline\n",
    "    rf_classifier = model.named_steps['classifier']\n",
    "    \n",
    "    # Get feature importances from the model\n",
    "    importances = rf_classifier.feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance_df = feature_importance_df.sort_values(\n",
    "        by='Importance', ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Display top features\n",
    "    print(\"\\nTop features by importance:\")\n",
    "    print(feature_importance_df.head(top_n))\n",
    "    \n",
    "    # Calculate permutation importance (more reliable)\n",
    "    print(\"\\nCalculating permutation importance (this may take a while)...\")\n",
    "    preprocessed_X_test = model.named_steps['preprocessor'].transform(X_test)\n",
    "    perm_importance = permutation_importance(\n",
    "        rf_classifier, preprocessed_X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Create a DataFrame for permutation importances\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Permutation_Importance_Mean': perm_importance.importances_mean,\n",
    "        'Permutation_Importance_Std': perm_importance.importances_std\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    perm_importance_df = perm_importance_df.sort_values(\n",
    "        by='Permutation_Importance_Mean', ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Display top features by permutation importance\n",
    "    print(\"\\nTop features by permutation importance:\")\n",
    "    print(perm_importance_df.head(top_n))\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(\n",
    "        x='Importance', y='Feature', \n",
    "        data=feature_importance_df.head(top_n),\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('Top Features by Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot permutation importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(\n",
    "        x='Permutation_Importance_Mean', y='Feature', \n",
    "        data=perm_importance_df.head(top_n),\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('Top Features by Permutation Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('permutation_importance.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return both importance DataFrames\n",
    "    return feature_importance_df, perm_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f148e",
   "metadata": {},
   "source": [
    "Section 5: Recommendation and Model Persistence Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea623fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recommend people based on availability\n",
    "def recommend_available_people(new_data, model, person_id_column=None, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend top N people based on predicted availability.\n",
    "    \n",
    "    Args:\n",
    "        new_data: DataFrame with features (same as training data)\n",
    "        model: Trained random forest pipeline\n",
    "        person_id_column: Column name containing person identifiers\n",
    "        top_n: Number of top recommendations to return\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with top N recommended people and probability scores\n",
    "    \"\"\"\n",
    "    print(f\"Generating top {top_n} recommendations...\")\n",
    "    \n",
    "    # Make a copy of the input data\n",
    "    data_copy = new_data.copy()\n",
    "    \n",
    "    # Extract person IDs if specified\n",
    "    person_ids = None\n",
    "    if person_id_column:\n",
    "        person_ids = data_copy[person_id_column].values\n",
    "        data_copy = data_copy.drop(person_id_column, axis=1)\n",
    "    else:\n",
    "        # Use index as person ID\n",
    "        person_ids = data_copy.index.values\n",
    "    \n",
    "    # Get probability of class 1 (available)\n",
    "    proba = model.predict_proba(data_copy)[:, 1]\n",
    "    \n",
    "    # Create results dataframe with ID and probability\n",
    "    results = pd.DataFrame({\n",
    "        'person_id': person_ids,\n",
    "        'availability_score': proba\n",
    "    })\n",
    "    \n",
    "    # Sort by probability descending and take top N\n",
    "    recommendations = results.sort_values(\n",
    "        by='availability_score', \n",
    "        ascending=False\n",
    "    ).head(top_n)\n",
    "    \n",
    "    print(\"Recommendations generated!\")\n",
    "    return recommendations\n",
    "\n",
    "# Function to save the model and related artifacts\n",
    "def save_model(model, feature_names, output_path='random_forest_availability_model.joblib'):\n",
    "    \"\"\"\n",
    "    Save the trained model and related artifacts.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        feature_names: List of feature names\n",
    "        output_path: Path to save the model\n",
    "    \"\"\"\n",
    "    print(f\"Saving model to {output_path}...\")\n",
    "    \n",
    "    model_artifacts = {\n",
    "        'model': model,\n",
    "        'feature_names': feature_names,\n",
    "        'creation_date': pd.Timestamp.now(),\n",
    "        'version': '1.0'\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_artifacts, output_path)\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "# Function to load the saved model\n",
    "def load_model(model_path='random_forest_availability_model.joblib'):\n",
    "    \"\"\"\n",
    "    Load a saved model.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        \n",
    "    Returns:\n",
    "        Loaded model and feature names\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    model_artifacts = joblib.load(model_path)\n",
    "    \n",
    "    model = model_artifacts['model']\n",
    "    feature_names = model_artifacts['feature_names']\n",
    "    \n",
    "    print(f\"Model loaded successfully! Version: {model_artifacts['version']}\")\n",
    "    return model, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769c6b0",
   "metadata": {},
   "source": [
    "Section 6: Main Function and Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18f4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the entire pipeline\n",
    "def main(file_path, target_column='assignment_valid', tune_hyperparams=True, save_model_path=None):\n",
    "    \"\"\"\n",
    "    Run the entire pipeline from data loading to model evaluation.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the data file\n",
    "        target_column: Name of the target column\n",
    "        tune_hyperparams: Whether to perform hyperparameter tuning\n",
    "        save_model_path: Path to save the model (if None, model will not be saved)\n",
    "        \n",
    "    Returns:\n",
    "        Trained model, feature names, and evaluation metrics\n",
    "    \"\"\"\n",
    "    # 1. Load and preprocess data\n",
    "    df = load_and_preprocess_data(file_path)\n",
    "    \n",
    "    # 2. Engineer features\n",
    "    df_engineered = engineer_features(df)\n",
    "    \n",
    "    # 3. Balance classes\n",
    "    df_balanced = balance_classes(df_engineered, target_column)\n",
    "    \n",
    "    # 4. Build and train model\n",
    "    model, feature_names, evaluation = build_and_train_model(\n",
    "        df_balanced, target_column, test_size=0.2, tune_hyperparams=tune_hyperparams\n",
    "    )\n",
    "    \n",
    "    # 5. Analyze feature importance\n",
    "    feature_importance_df, perm_importance_df = analyze_feature_importance(\n",
    "        model, feature_names, evaluation['X_test'], evaluation['y_test']\n",
    "    )\n",
    "    \n",
    "    # 6. Save model if specified\n",
    "    if save_model_path:\n",
    "        save_model(model, feature_names, save_model_path)\n",
    "        \n",
    "    print(\"\\nPipeline completed successfully!\")\n",
    "    return model, feature_names, evaluation\n",
    "\n",
    "# Example usage - modify this with your actual file path and options\n",
    "file_path = r\"C:\\Users\\ashwi\\Downloads\\sample_test_data.json\"  # Replace with your actual data file path\n",
    "\n",
    "# Run the pipeline (uncomment when ready to run)\n",
    "# model, feature_names, evaluation = main(\n",
    "#     file_path=file_path,\n",
    "#     target_column='assignment_valid',  # Change to your target column name if different\n",
    "#     tune_hyperparams=True,  # Set to False for faster results but potentially lower performance\n",
    "#     save_model_path='random_forest_availability_model.joblib'\n",
    "# )\n",
    "\n",
    "# Make recommendations (example)\n",
    "# new_data = pd.read_csv(\"new_people_data.csv\")  # Replace with your new data file\n",
    "# recommendations = recommend_available_people(new_data, model, person_id_column='employee_id', top_n=10)\n",
    "# print(\"\\nTop recommendations:\")\n",
    "# print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b4c2eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  employee_id          employee_skills  employee_availability task_id  \\\n",
      "0         E51  [db, frontend, backend]     [0, 1, 2, 4, 5, 6]    T159   \n",
      "1         E30     [frontend, ml, java]  [0, 1, 2, 3, 4, 5, 6]    T132   \n",
      "2         E16  [java, backend, db, ui]              [1, 4, 6]     T33   \n",
      "3         E38                 [python]        [0, 3, 4, 5, 6]    T188   \n",
      "4         E19            [backend, db]           [0, 2, 3, 5]      T9   \n",
      "\n",
      "  task_required_skills task_priority  task_duration_days  task_start_day  \\\n",
      "0               [java]        medium                   5               2   \n",
      "1       [java, ui, ml]           low                   5               4   \n",
      "2             [python]          high                   1               4   \n",
      "3             [python]        medium                   1               5   \n",
      "4               [java]           low                   4               3   \n",
      "\n",
      "   rule_violated  assignment_valid  \n",
      "0           True                 0  \n",
      "1           True                 0  \n",
      "2           True                 0  \n",
      "3          False                 1  \n",
      "4           True                 0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load from a JSON file\n",
    "df = pd.read_json(r\"C:\\Users\\ashwi\\OneDrive\\Desktop\\Sycamore\\Hackathons\\HackToFuture sjec\\ML part\\scheduling_dataset.json\")\n",
    "\n",
    "# If it's nested or has a specific structure, try:\n",
    "# df = pd.read_json(\"your_dataset.json\", orient=\"records\")  # or \"index\", \"split\", etc. depending on format\n",
    "\n",
    "print(df.head())  # üëà Just to check if it's loading properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca96df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  employee_id          employee_skills  employee_availability task_id  \\\n",
      "0         E51  [db, frontend, backend]     [0, 1, 2, 4, 5, 6]    T159   \n",
      "1         E30     [frontend, ml, java]  [0, 1, 2, 3, 4, 5, 6]    T132   \n",
      "2         E16  [java, backend, db, ui]              [1, 4, 6]     T33   \n",
      "3         E38                 [python]        [0, 3, 4, 5, 6]    T188   \n",
      "4         E19            [backend, db]           [0, 2, 3, 5]      T9   \n",
      "\n",
      "  task_required_skills task_priority  task_duration_days  task_start_day  \\\n",
      "0               [java]        medium                   5               2   \n",
      "1       [java, ui, ml]           low                   5               4   \n",
      "2             [python]          high                   1               4   \n",
      "3             [python]        medium                   1               5   \n",
      "4               [java]           low                   4               3   \n",
      "\n",
      "   rule_violated  assignment_valid  \n",
      "0           True                 0  \n",
      "1           True                 0  \n",
      "2           True                 0  \n",
      "3          False                 1  \n",
      "4           True                 0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(r\"C:\\Users\\ashwi\\OneDrive\\Desktop\\Sycamore\\Hackathons\\HackToFuture sjec\\ML part\\scheduling_dataset.json\")  # Replace with your actual path\n",
    "print(df.head())  # Confirm it's loading properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45caf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def preprocess_dataframe(df, fit_encoders=True, encoders=None):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if encoders is None:\n",
    "        encoders = {\n",
    "            \"skills\": MultiLabelBinarizer(),\n",
    "            \"availability\": MultiLabelBinarizer(),\n",
    "            \"task_skills\": MultiLabelBinarizer(),\n",
    "        }\n",
    "\n",
    "    if fit_encoders:\n",
    "        skills_encoded = encoders[\"skills\"].fit_transform(df['employee_skills'])\n",
    "        availability_encoded = encoders[\"availability\"].fit_transform(df['employee_availability'])\n",
    "        task_skills_encoded = encoders[\"task_skills\"].fit_transform(df['task_required_skills'])\n",
    "    else:\n",
    "        skills_encoded = encoders[\"skills\"].transform(df['employee_skills'])\n",
    "        availability_encoded = encoders[\"availability\"].transform(df['employee_availability'])\n",
    "        task_skills_encoded = encoders[\"task_skills\"].transform(df['task_required_skills'])\n",
    "\n",
    "    # Create DataFrames\n",
    "    skills_df = pd.DataFrame(skills_encoded, columns=[f\"skill_{s}\" for s in encoders[\"skills\"].classes_])\n",
    "    availability_df = pd.DataFrame(availability_encoded, columns=[f\"avail_day_{d}\" for d in encoders[\"availability\"].classes_])\n",
    "    task_skills_df = pd.DataFrame(task_skills_encoded, columns=[f\"task_req_{s}\" for s in encoders[\"task_skills\"].classes_])\n",
    "\n",
    "    # Drop original and unused columns\n",
    "    drop_cols = ['employee_skills', 'employee_availability', 'task_required_skills', 'employee_id', 'task_id']\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns])\n",
    "\n",
    "    # Concatenate\n",
    "    df_final = pd.concat([df.reset_index(drop=True), skills_df, availability_df, task_skills_df], axis=1)\n",
    "\n",
    "    return df_final, encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca268a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Column types BEFORE processing:\n",
      "employee_id              object\n",
      "employee_skills          object\n",
      "employee_availability    object\n",
      "task_id                  object\n",
      "task_required_skills     object\n",
      "task_priority            object\n",
      "task_duration_days        int64\n",
      "task_start_day            int64\n",
      "rule_violated              bool\n",
      "assignment_valid          int64\n",
      "dtype: object\n",
      "\n",
      "üìä Dataset shape: (500, 10)\n",
      "\n",
      "üîç Categorical Columns: ['employee_id', 'task_id', 'rule_violated', 'task_priority']\n",
      "üî¢ Numeric Columns: ['task_duration_days', 'task_start_day']\n",
      "üéØ List Columns (Multi-hot): ['employee_skills', 'employee_availability', 'task_required_skills']\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "‚úÖ Best parameters: {'classifier__max_depth': None, 'classifier__max_features': 'sqrt', 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        95\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n",
      "‚úÖ Model saved as 'smart_scheduler_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# üîß Set label column\n",
    "TARGET_COLUMN = 'assignment_valid'\n",
    "\n",
    "# üöÄ Load your JSON dataset\n",
    "df = pd.read_json(r\"C:\\Users\\ashwi\\OneDrive\\Desktop\\Sycamore\\Hackathons\\HackToFuture sjec\\ML part\\scheduling_dataset.json\")\n",
    "\n",
    "print(\"üîç Column types BEFORE processing:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# üßº Custom transformer for multi-hot encoding list fields\n",
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoder = MultiLabelBinarizer()\n",
    "        self.column_name = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.column_name = X.columns[0]\n",
    "        self.encoder.fit(X[self.column_name].str.split(','))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        col = self.column_name\n",
    "        values = X[col].str.split(',').fillna('')\n",
    "        transformed = self.encoder.transform(values)\n",
    "        new_cols = [f\"{col}__{c}\" for c in self.encoder.classes_]\n",
    "        return pd.DataFrame(transformed, columns=new_cols, index=X.index)\n",
    "\n",
    "\n",
    "# ‚úÖ Robust preprocessing function\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    list_columns = ['employee_skills', 'employee_availability', 'task_required_skills']\n",
    "    for col in list_columns:\n",
    "        if col in df.columns:\n",
    "            def to_string_list(x):\n",
    "                if isinstance(x, list):\n",
    "                    return ','.join(map(str, x))\n",
    "                elif isinstance(x, str):\n",
    "                    try:\n",
    "                        # Attempt to parse stringified list\n",
    "                        parsed = json.loads(x.replace(\"'\", '\"'))\n",
    "                        if isinstance(parsed, list):\n",
    "                            return ','.join(map(str, parsed))\n",
    "                    except:\n",
    "                        pass\n",
    "                return ''\n",
    "            df[col] = df[col].apply(to_string_list)\n",
    "    return df\n",
    "\n",
    "# üèóÔ∏è Build and train model\n",
    "def build_and_train_model(df_clean, label_column=TARGET_COLUMN):\n",
    "    print(\"\\nüìä Dataset shape:\", df_clean.shape)\n",
    "\n",
    "    if label_column not in df_clean.columns:\n",
    "        raise ValueError(f\"‚ùå Label column '{label_column}' not found. Available: {df_clean.columns.tolist()}\")\n",
    "\n",
    "    X = df_clean.drop(columns=[label_column])\n",
    "    y = df_clean[label_column]\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Identify columns\n",
    "    list_cols = ['employee_skills', 'employee_availability', 'task_required_skills']\n",
    "    # FIXED: Moved task_priority to categorical columns\n",
    "    categorical_cols = ['employee_id', 'task_id', 'rule_violated', 'task_priority']\n",
    "    # FIXED: Removed task_priority from numeric columns\n",
    "    numeric_cols = ['task_duration_days', 'task_start_day']\n",
    "\n",
    "    print(f\"\\nüîç Categorical Columns: {categorical_cols}\")\n",
    "    print(f\"üî¢ Numeric Columns: {numeric_cols}\")\n",
    "    print(f\"üéØ List Columns (Multi-hot): {list_cols}\")\n",
    "\n",
    "    # Column transformer\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('skills', MultiHotEncoder(), ['employee_skills']),\n",
    "        ('availability', MultiHotEncoder(), ['employee_availability']),\n",
    "        ('required_skills', MultiHotEncoder(), ['task_required_skills']),\n",
    "    ])\n",
    "\n",
    "    # Pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # GridSearch\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__max_depth': [None, 10],\n",
    "        'classifier__min_samples_split': [2, 5],\n",
    "        'classifier__max_features': ['sqrt']\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(pipe, param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(\"‚úÖ Best parameters:\", grid.best_params_)\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = grid.predict(X_test)\n",
    "    print(\"\\nüìã Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return grid.best_estimator_, X.columns.tolist()\n",
    "\n",
    "# üßπ Clean the data\n",
    "df_clean = preprocess_dataframe(df)\n",
    "\n",
    "# üß† Train the model\n",
    "model, used_features = build_and_train_model(df_clean, label_column=TARGET_COLUMN)\n",
    "\n",
    "# üíæ Save model\n",
    "joblib.dump(model, 'smart_scheduler_model.pkl')\n",
    "print(\"‚úÖ Model saved as 'smart_scheduler_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59d209a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_model.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"trained_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed19386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save feature names used during training\n",
    "with open(\"features.json\", \"w\") as f:\n",
    "    json.dump(features, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35a14158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV file 'sample_scheduling_dataset.csv' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Sample values\n",
    "skills = [\"python\", \"sql\", \"java\", \"ml\", \"excel\", \"communication\", \"db\", \"frontend\", \"backend\", \"ui\"]\n",
    "weekdays = list(range(7))  # 0 = Monday, 6 = Sunday\n",
    "priorities = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "data = {\n",
    "    \"employee_id\": [f\"emp_{i}\" for i in range(1, 21)],\n",
    "    \"employee_skills\": [random.sample(skills, k=random.randint(1, 4)) for _ in range(20)],\n",
    "    \"employee_availability\": [random.sample(weekdays, k=random.randint(2, 5)) for _ in range(20)],\n",
    "    \"task_id\": [f\"task_{i}\" for i in range(1, 21)],\n",
    "    \"task_required_skills\": [random.sample(skills, k=random.randint(1, 2)) for _ in range(20)],\n",
    "    \"task_priority\": random.choices(priorities, k=20),\n",
    "    \"task_duration_days\": [random.randint(1, 7) for _ in range(20)],\n",
    "    \"task_start_day\": [random.choice(weekdays) for _ in range(20)],\n",
    "    \"rule_violated\": random.choices([True, False], k=20),\n",
    "    \"assignment_valid\": random.choices([0, 1], k=20),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"sample_scheduling_dataset.csv\", index=False)\n",
    "print(\"‚úÖ CSV file 'sample_scheduling_dataset.csv' generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e9680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
